# AI Model Management RAG System Capabilities Demo

## Overview

ModelSearch AI is a powerful model search and retrieval system designed for machine learning researchers and engineers. The system enables intelligent searching across large model repositories, retrieving model information based on various criteria and providing structured outputs.

## Core Capabilities

### 1. Model Attribute-Based Retrieval

The system supports model retrieval based on the following attributes:

- **Architecture types**: CNN, RNN, UNet, etc.
- **Frameworks**: PyTorch, etc.
- **Datasets**: MNIST, Fashion-MNIST, Oxford Flower, etc.
- **Time ranges**: Filter by creation or modification date
- **Specific applications**: Such as game AI (2048 game)

**Example queries:**
```
Which models use Unet architectures?
Please find models created in March 2025 and using CNN.
Please find models created in April and using CelebA.
List all seq2seq models that use attention mechanisms.
Show me models trained on the MNIST dataset using autoencoder.
```

### 2. Specific Model Details Retrieval

The system provides detailed information about specific models, including:

- Model ID
- File size
- Creation date
- Last modification date
- Framework information
- Architecture details
- Training dataset
- Training configuration (batch size, learning rate, optimizer, etc.)
- Full description

**Example query:**
```
What DQN (Deep Q-Learning) agent did I try for the game called 2048?
Do I have models which were trained on animal images?
```

### 3. Model Details Comparison

The system allows users to compare detailed metadata between two or more models, enabling a clear understanding of architectural differences, training configurations, and performance metrics. This feature is essential for both researchers and engineers who need to evaluate model evolution, performance trade-offs, or dataset shifts.

**Capabilities:**
- Compare **architectures** (type, layers, hidden sizes, attention heads, parameters).
- Contrast **performance metrics** such as accuracy, loss, and perplexity.
- Analyze **training setups**, including batch size, optimizer, learning rate, hardware usage, and epochs.
- Examine **dataset usage** and framework details.
- Identify **relative improvements** across models (e.g., accuracy gains per parameter).
- Automatically generates structured comparisons such as best/worst rankings and pairwise performance gaps.

**Supported Comparison Types:**
- **Direct model ID comparison**: Compares multiple models by specifying their model IDs.
- **Cohort-based comparison**: Allows free-text cohort comparison, where each cohort is defined by a natural language query.

**Example queries:**
```
Please compare differences in architecture and dataset between model id model_123 and model id model_456.
Compare the diffusion models trained on CIFAR-10 and MNIST.
```

### 4. Generated Image Retrieval

The system supports retrieving images generated by specific models, providing detailed metadata:

- Image ID
- Parent model ID
- Creation date
- Last modification date
- Training epoch
- Image path

**Example query:**
```
please find images of model id oxford_flowers_conditional
```

### 5. Dataset Exploration

The system can explore various models trained on specific datasets:

**Example query:**
```
What diffusion models have been trained on the Oxford Flower dataset?
Show me models trained on the MNIST dataset using autoencoder.
```

## Output Formats

The system provides several formatted outputs:

1. **Tabular format**: Structured display of model information, including similarity scores, file sizes, creation dates, etc.
2. **Detailed descriptions**: Comprehensive information about model architecture, training parameters, and performance metrics
3. **Image listings**: Display of model-generated images and related metadata

## Use Case Scenarios

### Researcher Use Cases

- Find recent implementations of specific architectures
- Compare different models trained on the same dataset
- Track model training progress and results

### Engineer Use Cases

- Find pre-trained models suitable for specific applications
- Retrieve sample images generated by models
- Access model training configuration details to reproduce experiments

## Limitations and Notes

- Thumbnail display is not supported in CLI mode
- Image-to-image similarity search is not available in CLI mode
- Some complex semantic queries may require specific technical terminology for best results
- Search results may sometimes be truncated in responses

## Sameple Inputs/Outputs

### Sample 1

Input:
```
What diffusion models have been trained on the Oxford Flower dataset?
```
Output:
```
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
| Rank | Model ID        | Similarity Score | Similarity Distance |   Size | Created    | Modified   | Path            | Description          | Framework | Arch       | Dataset    | Batch | LR   | Optimizer | Epochs | HW  |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  1   | Generative-Oxfo |            0.235 |               0.174 | 76.7KB | 2025-04-04 | 2025-04-04 | /Users/yi-nungy | Navigate to the data | PyTorch 2 | Conditiona | Oxford-102 | 64    | 1e-0 | Adam      | 10     | CPU |
|      | rd-102-Flower_v |                  |                     |        |            |            | eh/PyCharmMiscP | folder's path        |           | lUNet      | Flower     |       | 3    |           |        |     |
|      | 2               |                  |                     |        |            |            | roject/Generati | (specified in        |           |            | Dataset (C |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Oxford-102-F | script), download    |           | The code   | onditional |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | lower/v2.py     | Oxford-102 Flowers   |           | combines   | Improved   |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset from its     |           | features   | v3)        |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | official website or  |           | of a convo |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | any other available  |           | lutional   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | source with high     |           | UNet and a |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | quality images of    |           | conditiona |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 256x256 resolution   |           | l          |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | saved as .png files. |           | denoising  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Saving Artifacts -   |           | diffusion  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The trained          |           | model.     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | autoencoder and      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | conditional          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising diffusion  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | network are saved at |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the completion of    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | each epoch using a   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | checkpoint callback  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | function within      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | PyTorch's learning   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | algorithm. Clone     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | this GitHub          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | repository or copy   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the entire codebase  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | into your working    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | environment to have  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the necessary        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | scripts, data        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | folders and models   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to start training    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and testing this     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | specific example: [h |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ttps://github.com/Yo |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ur-Username/Conditio |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | nal-Autoencoder-for- |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Oxford-102-Flower](h |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ttps://github.com/Yo |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ur-Username/Conditio |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | nal-Autoencoder-for- |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Oxford-102-Flower)   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 5. The AST           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | highlights the       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture of a    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | conditional color    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | classification and   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | autoencoder system   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | trained on the       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Oxford-102 Flowers   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset. Execute the |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | script using a       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Jupyter notebook or  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Python command line  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | interface: this will |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | train and evaluate   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | models on the        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset while        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | providing            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | visualizations like  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | loss curves,         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tensorboard plots,   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | image samples of     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | flowers and their    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | color labels to help |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | understand model     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | performance          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | effectively.         |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  2   | Generative-Oxfo |            0.218 |               0.288 | 63.1KB | 2025-04-04 | 2025-04-04 | /Users/yi-nungy | Data Pipeline and    | PyTorch 2 | Conditiona | Oxford-102 | 256   | 1e-0 | Adam      | 1000   | GPU |
|      | rd-102-Flower_v |                  |                     |        |            |            | eh/PyCharmMiscP | Preprocessing**      |           | lUNet      | Flowers    |       | 3    |           |        |     |
|      | 1               |                  |                     |        |            |            | roject/Generati |                      |           |            | Dataset    |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Oxford-102-F | The dataset consists |           | The code   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | lower/v1.py     | of the Oxford        |           | defines a  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Flowers              |           | Conditiona |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (flowers_train) and  |           | lUNEt      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Flowers Test         |           | class,     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | datasets for         |           | which      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training and         |           | combines a |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | testing,             |           | UNet archi |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | respectively.        |           | tecture    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Evaluation           |           | with       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Methodology**        |           | additional |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           | conditioni |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The script uses      |           | ng layers. |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | TensorBoard and Web- |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | based logging        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | frameworks           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (Matplotlib, PyPlot) |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to visualize         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training progress    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and evaluate model   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | performance metrics: |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - Time Embedding     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Visualization -      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Displays time        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | embeddings for class |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | features during a    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | specific growth      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | stage or lighting    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | condition. Purpose   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and Overview**       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The primary goal of  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | this script is to    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | build a conditional  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | generative           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | autoencoder (CGAE)   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for the Oxford       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Flowers dataset,     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | capable of learning  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class embeddings and |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | reconstructing input |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | images with high     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | accuracy while       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | incorporating        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | perceptual loss      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | during training.     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Model Architecture** |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The primary neural   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | network model used   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | in this script is    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the                  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | SimpleAutoencoder    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class, which         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consists of two      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | encoder and decoder  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | modules with         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | multiple custom      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | layers, including:   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - Swish module       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (inherits from       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Module), an          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | activation function  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for replacing ReLU.  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Test various         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | extensions by adding |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | new datasets or      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | modifying existing   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | configurations such  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | as batch             |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | normalization,       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dropout layers, etc. |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+

Model #1 - Generative-Oxford-102-Flower_v2:

* Model ID: Generative-Oxford-102-Flower_v2
* File Size: 78491
* Created On: 2025-04-04T21:50:47.672074
* Last Modified: 2025-04-04T21:50:47.672074
* Framework: PyTorch 2.7
* Architecture: ConditionalUNet

The code combines features of a convolutional UNet and a conditional denoising diffusion model. The architecture consists of three main parts - the encoder, the decoder, and the conditioning layers which are used for learning class embeddings.

* Dataset: Oxford-102 Flower Dataset (Conditional Improved v3)
* Training Configuration:
	+ Batch Size: 64
	+ Learning Rate: 0.001
	+ Optimizer: Adam
	+ Epochs: 10
	+ Hardware Used: CPU
* Description: Navigate to the data folder's path (specified in script), download Oxford-102 Flowers dataset from its official website or any other available source with high quality images of 256x256 resolution saved as .png files. Saving Artifacts - The trained autoencoder and conditional denoising diffusion network are saved at the completion of each epoch using a checkpoint callback function within PyTorch's learning algorithm. Clone this GitHub repository or copy the entire codebase into your working environment to have the necessary scripts, data folders and models to start training and testing this specific example: [https://github.com/Your-Username/Conditional-Autoencoder-for-Oxford-102-Flower](https://github.com/Your-Username/Conditional-Autoencoder-for-Oxford-102-Flower)
* The script uses TensorBoard and Web-based logging frameworks (Matplotlib, PyPlot) to visualize training progress and evaluate model performance metrics:
	+ Time Embedding Visualization - Displays time embeddings for class features during a specific growth stage or lighting condition.
	+ Data Pipeline and Preprocessing
	+ Training Configuration
	+ Evaluation Methodology
	
* Purpose and Overview: The primary goal of this script is to build a conditional generative autoencoder (CGAE) for the Oxford Flowers dataset, capable of learning class embeddings and reconstructing input images with high accuracy while incorporating perceptual loss during training. This model architecture aims to be suitable for generating flower images under various conditions or lighting stages by using conditioning layers that learn features specific to each stage or condition throughout its training process.
* Test various extensions by adding new datasets or modifying existing architecture configurations such as batch normalization, dropout layers, etc., in order to further improve the performance of this CGAE model on different flower classification tasks.

```

### Sample 2

Input:
```
Which models use Unet architectures?
```

Output:
```
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
| Rank | Model ID        | Similarity Score | Similarity Distance |   Size | Created    | Modified   | Path            | Description          | Framework | Arch       | Dataset    | Batch | LR   | Optimizer | Epochs | HW  |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  1   | Generative-     |            0.184 |               0.716 | 43.4KB | 2025-04-09 | 2025-03-16 | /Users/yi-nungy | The model uses the   | PyTorch 2 | SimpleUNet | Fashion    | 256   | 1e-0 | Adam      | 10     | GPU |
|      | Fashion-MNIST_l |                  |                     |        |            |            | eh/PyCharmMiscP | UNet architecture's  |           |            | MNIST      |       | 3    |           |        |     |
|      | atent_new_model |                  |                     |        |            |            | roject/Generati | final latent space   |           | The script |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Fashion-MNIS | representation to    |           | contains i |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | T/latent_new_mo | evaluate its         |           | mplementat |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | del.py          | performance by       |           | ions of    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | computing these      |           | UNET and U |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | metrics on test sets |           | NetAttenti |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | using                |           | onBlock    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `TestDataLoader`.    |           | classes,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `SimpleUNet`: A UNet |           | along with |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture with    |           | other      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | input channels equal |           | related    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to 1, hidden         |           | modules    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dimensions [32, 64,  |           | such as    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 128], and five       |           | Encoder,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | residual blocks (`Un |           | Decoder,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | etResidualBlock`).   |           | and UNetRe |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The model is trained |           | sidualBloc |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | using the Adam       |           | k. Additio |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | optimizer with       |           | nally, it  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning rate        |           | features S |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | LR=0.0002 and weight |           | impleAutoe |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | decay WD=0.0005 on a |           | ncoder     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | device of either CPU |           | class      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | or NVIDIA GPU        |           | which      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | depending on their   |           | inherits   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | availability         |           | from       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (`torch.device('mps' |           | Module.    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | if torch.mps.is_avai |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | lable() else         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 'cpu')`). Training   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Configuration**      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Training             |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | configuration        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | includes the         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | following            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | parameters:          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - `optimizer`: The   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Adam optimizer is    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | used for all models  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | with initial         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning rates set   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to LR=0.0002 and     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | weight decay of      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | WD=0.0005. - `loss`: |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | PyTorch's loss       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | module is used for   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | calculating losses   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | during training,     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | including            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | contrastive loss for |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | autoencoder training |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and Focal Losses     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | with Softplus        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | activation functions |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to handle unbalanced |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | datasets in          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising diffusion  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | processes (Fashion   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | MNIST).              |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  2   | Generative-Oxfo |            0.154 |               0.768 | 63.1KB | 2025-04-04 | 2025-04-04 | /Users/yi-nungy | The decoder is an    | PyTorch 2 | Conditiona | Oxford-102 | 256   | 1e-0 | Adam      | 1000   | GPU |
|      | rd-102-Flower_v |                  |                     |        |            |            | eh/PyCharmMiscP | UNet architecture    |           | lUNet      | Flowers    |       | 3    |           |        |     |
|      | 1               |                  |                     |        |            |            | roject/Generati | with several custom  |           |            | Dataset    |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Oxford-102-F | modules such as:     |           | The code   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | lower/v1.py     |                      |           | defines a  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - UNetAttentionBlock |           | Conditiona |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (inherits from       |           | lUNEt      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Sequential), which   |           | class,     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | adds a skip          |           | which      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | connection between a |           | combines a |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | convolution layer    |           | UNet archi |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and a transposed     |           | tecture    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | convolution layer    |           | with       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for residual         |           | additional |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | connections within   |           | conditioni |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | each UNet block.     |           | ng layers. |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Evaluation           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Methodology**        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The script uses      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | TensorBoard and Web- |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | based logging        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | frameworks           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (Matplotlib, PyPlot) |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to visualize         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training progress    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and evaluate model   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | performance metrics: |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - Time Embedding     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Visualization -      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Displays time        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | embeddings for class |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | features during a    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | specific growth      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | stage or lighting    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | condition. Training  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Methodology**        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The script uses      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | TensorFlow's Keras   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | API for deep         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning model       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | construction and the |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | PyTorch framework to |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | handle GPU           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | computations when    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | available. Model     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Architecture**       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The primary neural   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | network model used   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | in this script is    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the                  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | SimpleAutoencoder    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class, which         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consists of two      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | encoder and decoder  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | modules with         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | multiple custom      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | layers, including:   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - Swish module       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (inherits from       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Module), an          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | activation function  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for replacing ReLU.  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Training             |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Configuration**      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The training process |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consists of several  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | optimizers, learning |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | rates, loss          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | functions, and       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | evaluation metrics:  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - Optimizer -        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Stochastic Gradient  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Descent (Adam) with  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | L2 regularization on |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | weights.             |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  3   | Generative-     |            0.141 |               0.225 | 33.8KB | 2025-03-14 | 2025-03-14 | /Users/yi-nungy | However, some areas  | PyTorch 2 | UnetDenois | Fashion    | 128   | 2e-0 | AdamW     | 150    | CPU |
|      | Fashion-MNIST_s |                  |                     |        |            |            | eh/PyCharmMiscP | that could improve   |           | er         | MNIST      |       | 4    |           |        |     |
|      | cript_unet      |                  |                     |        |            |            | roject/Generati | or extend this model |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Fashion-MNIS | include adjusting    |           | The model  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | T/script_unet.p | hyperparameters for  |           | is a       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | y               | better learning,     |           | modified   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | implementing data    |           | U-Net arch |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | augmentation         |           | itecture s |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | techniques to        |           | pecificall |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | increase training    |           | y designed |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | diversity, and fine- |           | for        |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tuning the UNet      |           | denoising  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture based   |           | Fashion    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | on certain denoising |           | MNIST      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | criteria (e.g.,      |           | images. It |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | minimal amount of    |           | consists   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | noise). The script   |           | of several |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ends with a device   |           | residual   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | setting based on     |           | blocks,    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | whether GPU or MPS   |           | patch conv |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (if available)       |           | olutional  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | should be used for   |           | layers,    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | computation, and     |           | and a      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | then loads the Fashi |           | self-      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | onMNISTUNetDenoiser  |           | attention  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class onto it which  |           | mechanism. |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | is initialized using |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | certain parameters   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | such as image size,  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | input channels,      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | number of output     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | classes, time        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dimension, and patch |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | size. Lastly, the    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | optimizer AdamW with |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | a learning rate at   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 0.0002, weight decay |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | at 0.02, and betas   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | at (0.9, 0.99) is    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | assigned to train    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | this model through   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | epochs. FashionMNIST |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | UNetDenoiser class   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | inherits from        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Module, which        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | provides the overall |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | structure of the     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | U-Net architecture   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for denoising        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | grayscale images     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | using a time         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dimension and patch  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | size parameters. It  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | uses PyTorch as the  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | main framework, and  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | trains a model on a  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset containing   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 60,000 grayscale     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | images of fashion    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | items.               |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+

### EXECUTIVE SUMMARY

This report presents a detailed analysis of three models that use variations of the Unet architecture: Generative-Fashion-MNIST_script_unet, Generative-Oxford-102-Flower_v1, and Generative-Fashion-MNIST_latent_new_model. These models employ UNet or ConditionalUnet architectures for various tasks such as image denoising, flower classification, and fashion item recognition using grayscale images. The three models have distinct features, hyperparameters, frameworks, and datasets.

### TECHNICAL SPECIFICATIONS

Model 1: Generative-Fashion-MNIST_latent_new_model
* Architecture: SimpleUNet
	+ Input channels: Equal to 1
	+ Hidden dimensions: [32, 64, 128]
	+ Residual blocks: UNetResidualBlock (5)
	* Framework: PyTorch 2.7
* Dataset: Fashion MNIST
* Training Configuration:
	+ Batch Size: 256
	+ Learning Rate: 0.001
	+ Optimizer: Adam
	+ Epochs: 10
	+ Hardware Used: GPU (or CPU)

Model 2: Generative-Oxford-102-Flower_v1
* Architecture: ConditionalUNet
	+ Decoder with conditioning layers for flower classification
	* Dataset: Oxford-102 Flowers Dataset
* Framework: PyTorch 2.7
* Training Configuration:
	+ Batch Size: 256
	+ Learning Rate: 0.001
	+ Optimizer: Adam
	+ Epochs: 1000
	+ Hardware Used: GPU (or CPU)

Model 3: Generative-Fashion-MNIST_script_unet
* Architecture: UNetDenoiser
	+ Modified U-Net architecture for grayscale image denoising
	* Dataset: Fashion MNIST
* Framework: PyTorch 2.7
* Training Configuration:
	+ Batch Size: 128
	+ Learning Rate: 0.0002
	+ Optimizer: AdamW
	+ Epochs: 150
	+ Hardware Used: CPU (or GPU)

### IMPLEMENTATION DETAILS

Each model has its distinct implementation details, such as the number of layers and parameters in UNet or ConditionalUNet architectures. The SimpleAutoencoder class in Model 1 employs the aforementioned architecture with a few adjustments for autoencoding tasks. Similarly, conditional branches are included in the ConditionalUNet architecture used in Model 2 to handle flower classification.

### PERFORMANCE ANALYSIS

Each model offers different performance metrics based on their respective architectures and datasets. The Generative-Fashion-MNIST_latent_new_model achieves better denoising results, while the other two models excel in classifying flowers using the Fashion MNIST dataset.

### TECHNICAL INSIGHTS

These three UNet or ConditionalUnet variations showcase how a well-structured neural network architecture can be adapted to address various tasks like image denoising and classification by incorporating additional conditioning layers in conditional networks. However, further enhancements could include adjusting hyperparameters for better learning, implementing data augmentation techniques to increase training diversity, and fine-tuning the UNet architecture based on certain performance criteria (e.g., minimal amount of noise).

```

### Sample 3

Input:
```
Can you please compare differences in architecture between my diffusion models on CIFAR-10 and MNIST?
```

Output:
```
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
| Rank | Model ID        | Similarity Score | Similarity Distance |   Size | Created    | Modified   | Path            | Description          | Framework | Arch       | Dataset    | Batch | LR   | Optimizer | Epochs | HW  |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  1   | Gnerative-      |            0.618 |               0.219 | 58.2KB | 2025-04-01 | 2025-04-01 | /Users/yi-nungy | **Abstract Syntax    | PyTorch 2 | Conditiona | CIFAR10Cat | 256   | 1e-0 | Adam      | 1000   | CPU |
|      | CIFAR-10-GAN_v1 |                  |                     |        |            |            | eh/PyCharmMiscP | Tree (AST) Digest    |           | lUNet      | Dog        |       | 4    |           |        |     |
|      |                 |                  |                     |        |            |            | roject/Gnerativ | Summary**            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | e-CIFAR-10-GAN/ |                      |           | The Condit |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | v1.py           | The provided AST     |           | ionalUNet  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | digest summary       |           | class      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | exhibits a complex   |           | (inherits  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | machine learning     |           | from       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture,        |           | Module) is |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consisting of        |           | identified |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | several components   |           | in the AST |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | that work together   |           | digest     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to train and improve |           | summary,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | an image             |           | along with |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | classification model |           | other      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for the              |           | residual   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | CIFAR10CatDog        |           | blocks and |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset. The model's |           | attention  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture can be  |           | mechanisms |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | extended by altering |           | .          |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the hyperparameters  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | or experimenting     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | with different loss  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | functions to improve |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | performance. The     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset is also      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | divided into         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training,            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | validation, and      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | testing sets.        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | **Reproduction and   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Extension Guide**    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | To reproduce this    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | script, the reader   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | should have all      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | necessary Python     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | libraries installed  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (e.g., PyTorch,      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | TensorFlow), as well |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | as a working CUDA    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | environment if       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | desired GPU          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | acceleration is      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | needed. The model is |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | trained using        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | PyTorch's AdamW      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | optimizer with       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | various              |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | hyperparameters like |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning rate (lr),  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | weight decay         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (weight_decay),      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | betas, and other     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | device-specific      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | configurations.      |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  2   | Generative-     |            0.382 |               0.117 | 16.9KB | 2025-03-05 | 2025-03-05 | /Users/yi-nungy | Further              | PyTorch 2 | OptimizedD | Fashion    | 256   | 5e-0 | AdamW     | 15     | CPU |
|      | Fashion-MNIST_t |                  |                     |        |            |            | eh/PyCharmMiscP | modifications        |           | enoiser    | MNIST      |       | 4    |           |        |     |
|      | ransformer_new  |                  |                     |        |            |            | roject/Generati | include implementing |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Fashion-MNIS | a set of custom      |           | The Optimi |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | T/transformer_n | transformations for  |           | zedDenoise |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ew.py           | both training and    |           | r class    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | validation datasets  |           | inherits   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | using PyTorch's      |           | from       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `DataLoader`         |           | Module,    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | utility.             |           | indicating |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Visualization and    |           | a          |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Output Artifacts     |           | denoising  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The visualization    |           | architectu |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | part includes        |           | re with    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | plotting various     |           | encoder-   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | metrics like MSE     |           | decoder    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | loss over epochs to  |           | modules    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | monitor model        |           | and other  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | performance during   |           | distinctiv |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | optimization.        |           | e implemen |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Extensions:          |           | tation     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | a) Modify the        |           | patterns.  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture of      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | OptimizedDenoiser    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class for different  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tasks (e.g.,         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising real       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | images from          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ImageNet). Model     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Architecture         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The model            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consists of an       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | OptimizedDenoiser    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class that inherits  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | from torch's         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `Module` base class. |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Purpose and Overview |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The script presented |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | here is an optimized |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising generative |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | model for Fashion    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | MNIST dataset, which |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | employs PyTorch      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | library (version     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 1.8.0) with CUDA     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 11.3 support.        |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+

**Model #1: Gnerative-CIFAR-10-GAN_v1**
--------------------------------------------

Architecture: ConditionalUNet

The provided model utilizes the ConditionalUNet architecture, which inherits from PyTorch's `Module` class. This complex machine learning model consists of several components that work together to train and improve an image classification model for the CIFAR10CatDog dataset. The following are its main features:

- **ConditionalUNet**: A deep learning architecture combining UNet with a conditional layer on top. It is designed specifically for segmentation tasks with additional information, such as multi-modalities or different scales. This architecture aims to learn from sparse annotations by handling the complex and diverse requirements of image classification in CIFAR10CatDog dataset.

**Dataset:** CIFAR10CatDog

The CIFAR10CatDog dataset is a combination of images obtained from the CIFAR10 dataset, which consists of 60000 color images with labels for training (50000) and testing/validation datasets (10000). It also adds a new category called 'background' to better handle cats-and-dogs images in diverse conditions.

**Training Configuration:**

* Batch Size: 256
* Learning Rate: 0.0001
* Optimizer: Adam
* Epochs: 1000
* Hardware Used: CPU

The model is trained using PyTorch's `AdamW` optimizer with various hyperparameters such as learning rate (lr), weight decay (`weight_decay`), betas, and other device-specific configurations. Training this architecture requires a powerful GPU to ensure the training process runs smoothly and efficiently.

**Description:** Abstract Syntax Tree Digest Summary

The provided AST digest summary exhibits a complex machine learning architecture that can be extended by altering the hyperparameters or experimenting with different loss functions to improve performance. The dataset is also divided into training, validation, and testing sets. 

**Reproduction and Extension Guide:**

To reproduce this script, the reader should have all necessary Python libraries installed (e.g., PyTorch, TensorFlow), as well as a working CUDA environment if desired GPU acceleration is needed. The model is trained using PyTorch's AdamW optimizer with various hyperparameters like learning rate  (lr), weight decay  (weight_decay), betas, and other device-specific configurations.

**Model #2: Generative-Fashion-MNIST\_transformer\_new**
----------------------------------------------------------

Architecture: OptimizedDenoiser

The provided model employs the `OptimizedDenoiser` class, which inherits from PyTorch's `Module` base class. This architecture is an optimized denoising generative model for Fashion MNIST dataset. The following are its main features:

- **OptimizedDenoiser**: A deep learning architecture combining encoder and decoder modules within the same layer to perform image denoising tasks, featuring a custom loss function that enables supervised training using FashionMNIST as input data. 

**Dataset:** Fashion MNIST

The Fashion MNIST dataset is a collection of images with 60000 grayscale images representing fashion products such as sneakers, dresses, hats, and more. The dataset has five classes in total (t-shirt/top, trouser, pullover, dress, coat). Each class contains different styles and colors of clothing items that need to be accurately recognized by the model.

**Training Configuration:**

* Batch Size: 256
* Learning Rate: 0.0005
* Optimizer: AdamW
* Epochs: 15
* Hardware Used: CPU

The model is trained using PyTorch's `AdamW` optimizer with various hyperparameters such as learning rate (lr), weight decay (`weight_decay`), betas, and other device-specific configurations. Training this architecture requires a powerful GPU to ensure the training process runs smoothly and efficiently.

**Description:** Further Visualizations and Output Artifacts

The visualization part includes plotting various metrics like MSE loss over epochs to monitor model performance during optimization. Extensions: 
a) Modify the architecture of OptimizedDenoiser class for different tasks (e.g., denoising real images from ImageNet).
```

### Sample 4

Input:
```
What DQN (Deep Q-Learning) agent did I try for the game called 2048
```

Output:
```
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+----------+-------+------+-----------+--------+-----+
| Rank | Model ID        | Similarity Score | Similarity Distance |   Size | Created    | Modified   | Path            | Description          | Framework | Arch       | Dataset  | Batch | LR   | Optimizer | Epochs | HW  |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+----------+-------+------+-----------+--------+-----+
|  1   | GamePlay_script |            0.390 |               0.464 | 28.3KB | 2025-03-05 | 2025-03-05 | /Users/yi-nungy | **Model              | PyTorch 2 | Sequential | Game2048 | 32    | 1e-0 | Adam      | 10     | GPU |
|      | 2048            |                  |                     |        |            |            | eh/PyCharmMiscP | Architecture:**      |           | with DQN   |          |       | 3    |           |        |     |
|      |                 |                  |                     |        |            |            | roject/GamePlay | The model consists   |           |            |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | /script2048.py  | of two main          |           | The model  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | components: the DQN  |           | is a       |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | agent, which learns  |           | sequential |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | policies for playing |           | architectu |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the 2048 game, and   |           | re built   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the Deep Q-Learning  |           | using      |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (DQL) network, which |           | PyTorch, i |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | serves as an         |           | ncorporati |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | approximator of the  |           | ng a       |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | value function. The  |           | Dueling    |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | evaluation process   |           | Deep Q     |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | involves playing     |           | Network (D |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | games in order to    |           | QNetwork)  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | measure performance  |           | for        |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | metrics such as the  |           | learning   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | average score        |           | and        |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | achieved and whether |           | decision-  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the DQN agent has    |           | making in  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | memorized tile       |           | the game.  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | combinations. The    |           | The        |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | DQNetwork has a one- |           | primary    |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | hot encoded input    |           | components |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | representing the     |           | are        |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | state of the game    |           | Game2048   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | grid and outputs a   |           | class      |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | vector of values     |           | which      |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | indicating the       |           | inherits   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | expected future      |           | from       |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | reward for each      |           | nn.Module, |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | action taken.        |           | DQNAgent   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Additionally,        |           | class with |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | install DQNetwork    |           | features   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and Game2048 classes |           | such as re |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | from the code using  |           | member(),  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | pip:                 |           | act()      |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | ```python            |           | etc., and  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | pip install -e . --- |           | layers of  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Game2048 ML Script   |           | DQNAgent   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Documentation        |           | including  |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Specialist ---       |           | input_shap |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           | e, state_s |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | **Purpose:** The     |           | hape, acti |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | purpose of this      |           | on_size,   |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | script is to train a |           | etc.       |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Deep Q-Learning      |           |            |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | agent on the 2048    |           |            |          |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | game.                |           |            |          |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+----------+-------+------+-----------+--------+-----+

### Executive Summary

This report describes a DQN (Deep Q-Learning) agent used for playing the 2048 game, as found in Model ID [GamePlay_script2048](https://github.com/ml-gameplay-scripts/DQNAgent). The model is built using PyTorch framework and follows a sequential architecture with DQN as the primary component.

Key technical characteristics of this agent include:
1. Framework: PyTorch 2.7
2. Architecture: Sequential with DQN (Deep Q-Learning) network as the learning and decision-making component.
3. Dataset: Game2048, a class representing the game grid state for training and evaluation.
4. Training Configuration: Batch Size of 32, Learning Rate of 0.001, Optimizer set to Adam, Epochs at 10, using GPU hardware.
5. Evaluation Process: Evaluates performance based on average score achieved in playing games, memorization of tile combinations.
6. Python Package Installation: Install DQNetwork and Game2048 classes from the code using pip by running `pip install -e .` at the root directory.

### Technical Specifications

#### Model Architecture
The agent comprises a DQN network as the primary learning component, with one-hot encoded input for representing game grid states and outputting value vectors indicating future reward expectations per action taken. The evaluation process involves playing games to measure performance metrics like average score achieved and memorization of tile combinations.

#### Components & Configuration
The agent consists of:
1. Game2048 class: Represents the 2048 game grid state for training and evaluation purposes.
2. DQNAgent class: Contains features such as remember() and act(), acting as a container for DQNetwork, which implements Deep Q-Learning.
3. DQNetwork layers: Includes input_shape (representing tile values) and state_shape (representing the current game grid). The action_size represents valid actions per state in this case, typically [0, 1].
4. Training Configuration: Batch Size of 32, Learning Rate of 0.001, Optimizer set to Adam, Epochs at 10, using GPU hardware for training.

### Implementation Details

#### Environment Setup & Dependencies
To run this agent and replicate the results, you'll need:
1. Python >=3.6 version installed on your system (with pip).
2. PyTorch framework installed via pip (`pip install torch`).
3. CUDA Toolkit for GPU acceleration if using a machine with Nvidia hardware.
4. NVIDIA drivers installed properly to maximize GPU performance in Python environments.
5. Git CLI available, as this agent is stored on GitHub and can be cloned from there using `git clone <repo-url>`.

### Performance Analysis
Performance metrics are not explicitly provided within the search result; however, it's reasonable to assume that playing games with the DQN agent should provide an average score achieved which indicates its performance. If needed, you may need additional data or a longer training process using this configuration before evaluating and comparing against other agents/methods.

### Technical Insights & Opportunities
This Deep Q-Learning (DQN) agent represents a common implementation approach for playing the 2048 game, with PyTorch as its chosen framework. The sequential architecture reduces complexity while allowing learning from the environment's state transitions. Further exploration of DQNetwork and Game2048 classes may provide insights into how Q-Learning works in practice within this specific context.

### Information Gaps & Considerations
1. Limitation: No explicit mention is available about when to stop training or evaluating the agent, which would impact learning progress and performance results over time. It's suggested that you perform additional experiments with varying hyperparameters for a more comprehensive understanding of its limitations and potential optimizations.
2. Suggested Improvements: To improve this implementation, consider leveraging experience replay (a technique designed to reduce variance in DQN) or other advanced Q-Learning algorithms for better performance. Additionally, hyperparameter tuning could be conducted using techniques like Grid Search or Random Search to find the optimal configuration for this agent's learning process and performance results.
3. Additional Resources: If you want further understanding of how DQNetwork works within a Deep Learning context, consider exploring other similar models and related resources from reputable sources on GitHub or academic research publications about deep Q-Learning algorithms. 
```

### Sample 5

Input:
```
please compare architecture and training config of model id Generative-Fashion-MNIST_latent_new_model and model id Gnerative-CIFAR-10-GAN_v1
```

Output:
```
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
| Rank | Model ID        | Similarity Score | Similarity Distance |   Size | Created    | Modified   | Path            | Description          | Framework | Arch       | Dataset    | Batch | LR   | Optimizer | Epochs | HW  |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  1   | Gnerative-      |            0.565 |               2.000 | 58.2KB | 2025-04-01 | 2025-04-01 | /Users/yi-nungy | Additionally, the    | PyTorch 2 | Conditiona | CIFAR10Cat | 256   | 1e-0 | Adam      | 1000   | CPU |
|      | CIFAR-10-GAN_v1 |                  |                     |        |            |            | eh/PyCharmMiscP | script plots         |           | lUNet      | Dog        |       | 4    |           |        |     |
|      |                 |                  |                     |        |            |            | roject/Gnerativ | generated samples    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | e-CIFAR-10-GAN/ | side by side with    |           | The Condit |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | v1.py           | real images to       |           | ionalUNet  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | visually evaluate    |           | class      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the quality of GAN   |           | (inherits  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | output. **Purpose    |           | from       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and Overview**       |           | Module) is |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The main objective   |           | identified |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | of this code is to   |           | in the AST |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | train a generative   |           | digest     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | adversarial network  |           | summary,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (GAN) that can       |           | along with |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | generate new images  |           | other      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | from the             |           | residual   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | CIFAR10CatDog        |           | blocks and |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset by           |           | attention  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | conditioning on the  |           | mechanisms |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | original training    |           | .          |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | data. Important      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dependencies include |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | torchvision for data |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | augmentation and     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | preprocessing, tqdm  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for progress         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tracking, imageio    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for viewing          |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | generated images,    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and numpy & os for   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | various              |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | computations.        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | **Visualization and  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Output Artifacts**   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The code generates a |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | series of            |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tensorboard files    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | (.pt) containing     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | various metrics      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | throughout training, |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | along with           |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | visualizations using |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | matplotlib's pyplot  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | library for detailed |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | monitoring of loss   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | curves, feature      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | embeddings, and      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | generated image      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | samples. **Abstract  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Syntax Tree (AST)    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Digest Summary**     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 |                      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | The provided AST     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | digest summary       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | exhibits a complex   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | machine learning     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture,        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | consisting of        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | several components   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | that work together   |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to train and improve |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | an image             |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | classification model |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | for the              |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | CIFAR10CatDog        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dataset.             |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+
|  2   | Generative-     |            0.435 |               2.000 | 43.4KB | 2025-04-09 | 2025-03-16 | /Users/yi-nungy | - `loss`: PyTorch's  | PyTorch 2 | SimpleUNet | Fashion    | 256   | 1e-0 | Adam      | 10     | GPU |
|      | Fashion-MNIST_l |                  |                     |        |            |            | eh/PyCharmMiscP | loss module is used  |           |            | MNIST      |       | 3    |           |        |     |
|      | atent_new_model |                  |                     |        |            |            | roject/Generati | for calculating      |           | The script |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | ve-Fashion-MNIS | losses during        |           | contains i |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | T/latent_new_mo | training, including  |           | mplementat |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            | del.py          | contrastive loss for |           | ions of    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | autoencoder training |           | UNET and U |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and Focal Losses     |           | NetAttenti |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | with Softplus        |           | onBlock    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | activation functions |           | classes,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to handle unbalanced |           | along with |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | datasets in          |           | other      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | denoising diffusion  |           | related    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | processes (Fashion   |           | modules    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | MNIST).              |           | such as    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `SimpleAutoencoder`: |           | Encoder,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | A simple autoencoder |           | Decoder,   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | with input channels  |           | and UNetRe |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | set to 1 (grayscale  |           | sidualBloc |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | images) and latent   |           | k. Additio |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dimensions equal to  |           | nally, it  |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 64. - `scheduler`: A |           | features S |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning rate        |           | impleAutoe |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | scheduler based on   |           | ncoder     |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | the OneCyclePolicy   |           | class      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | class from           |           | which      |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `torch.optim`        |           | inherits   |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | manages learning     |           | from       |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | rates during the     |           | Module.    |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training process.    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | Key evaluation       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | metrics are:         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | - `epoch_loss`: A    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training loss        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | tracker used to      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | monitor how well     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | each model is        |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | learning during its  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | training process,    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | and a cumulative     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | score calculated     |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | from the average of  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | epoch losses for all |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | models per each      |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | epoch (using         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | function 'epoch_loss |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | += loss.item()').    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | `SimpleUNet`: A UNet |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | architecture with    |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | input channels equal |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | to 1, hidden         |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | dimensions [32, 64,  |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | 128], and five       |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | residual blocks (`Un |           |            |            |       |      |           |        |     |
|      |                 |                  |                     |        |            |            |                 | etResidualBlock`).   |           |            |            |       |      |           |        |     |
+------+-----------------+------------------+---------------------+--------+------------+------------+-----------------+----------------------+-----------+------------+------------+-------+------+-----------+--------+-----+

### Executive Summary

This report compares the architecture and training configurations of two machine learning models: Generative-Fashion-MNIST_latent_new_model (Model ID: Gnerative-Fashion-MNIST_latent_new_model) and Gnerative-CIFAR-10-GAN_v1 (Model ID: Gnerative-CIFAR-10-GAN_v1). Both models utilize PyTorch as their framework, with different architectures tailored for specific datasets.

### Technical Specifications

**Generative-Fashion-MNIST\_latent\_new\_model:**
* Model ID: Generative-Fashion-MNIST\_latent\_new_model
* File Size: 44421
* Created On: 2025-03-16T15:06:40.737226
* Last Modified: 2025-04-09T11:17:44.630196
* Framework: PyTorch 2.7
* Architecture: SimpleUNet
	* The script contains implementations of UNET and UNetAttentionBlock classes, along with other related modules such as Encoder, Decoder, and UNetResidualBlock.
* Dataset: Fashion MNIST
* Training Configuration:
	+ Batch Size: 256
	+ Learning Rate: 0.001
	+ Optimizer: Adam
	+ Epochs: 10
	+ Hardware Used: GPU
	+ `loss`: PyTorch's loss module is used for calculating losses during training, including contrastive loss for autoencoder training and Focal Losses with Softplus activation functions to handle unbalanced datasets in denoising diffusion processes (Fashion MNIST).
* SimpleAutoencoder: A simple autoencoder with input channels set to 1 (grayscale images) and latent dimensions equal to 64.
	+ `scheduler`: A learning rate scheduler based on the OneCyclePolicy class from 'torch.optim' manages learning rates during the training process.
	+ Key evaluation metrics are:
		- epoch_loss: A training loss tracker used to monitor how well each model is learning during its training process, and a cumulative score calculated from the average of epoch losses for all models per each epoch (using function 'epoch_loss += loss.item()').
* SimpleUNet: A UNet architecture with input channels equal to 1, hidden dimensions [32, 64, 128], and five residual blocks (UnetResidualBlock).

**Gnerative-CIFAR-10-GAN\_v1:**
* Model ID: Gnerative-CIFAR-10-GAN_v1
* File Size: 59600
* Created On: 2025-04-01T07:43:03.142255
* Last Modified: 2025-04-01T07:43:03.142255
* Framework: PyTorch 2.7
* Architecture: ConditionalUNet
	* The script contains implementations of UNet and Attention mechanisms, along with other related modules such as residual blocks and attention mechanisms. It also has a custom DatasetLoader class for data loading.
* Dataset: CIFAR10CatDog
* Training Configuration:
	+ Batch Size: 256
	+ Learning Rate: 0.0001
	+ Optimizer: Adam
	+ Epochs: 1000
	+ Hardware Used: CPU
* Description:
	+ The main objective of this code is to train a generative adversarial network (GAN) that can generate new images from the CIFAR10CatDog dataset by conditioning on the original training data.
	+ Important dependencies include torchvision for data augmentation and preprocessing, tqdm for progress tracking, imageio for viewing generated images, and numpy & os for various computations.
	+ Additionally, the script generates a series of tensorboard files (.pt) containing various metrics throughout training, along with visualizations using matplotlib's pyplot library for detailed monitoring of loss curves, feature embeddings, and generated image samples.

```
