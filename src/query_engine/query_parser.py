import logging
import re
from enum import Enum
from typing import Dict, List, Any, Optional, Union

import nltk
import spacy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Try to import sentence-transformers if installed
try:
    from sentence_transformers import SentenceTransformer

    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Try to import Hugging Face transformers and onnxruntime for local LLM inference
try:
    import transformers
    import onnxruntime

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


# Define intent types as an Enum for type safety
class QueryIntent(Enum):
    RETRIEVAL = "retrieval"  # Basic information retrieval
    COMPARISON = "comparison"  # Model comparison
    NOTEBOOK = "notebook"  # Notebook generation
    IMAGE_SEARCH = "image_search"  # Image search/retrieval
    METADATA = "metadata"  # Metadata-specific queries
    UNKNOWN = "unknown"  # Unknown/ambiguous intent


class QueryParser:
    """
    Parser for natural language queries related to AI models.
    Responsible for intent classification and parameter extraction.
    """

    def __init__(self, nlp_model: str = "en_core_web_sm", use_langchain: bool = True, llm_model_name: str = "deepseek-llm:7b"):
        """
        Initialize the QueryParser with necessary NLP components.

        Args:
            nlp_model: The spaCy model to use for NLP tasks
            use_langchain: Whether to use LangChain for enhanced parsing
        """
        # Set up logging
        self.logger = logging.getLogger(__name__)

        # Initialize NLP components
        try:
            self.nlp = spacy.load(nlp_model)
            self.logger.info(f"Loaded spaCy model: {nlp_model}")
        except OSError:
            self.logger.warning(f"Could not load spaCy model: {nlp_model}. Running spacy download...")
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", nlp_model], check=True)
            self.nlp = spacy.load(nlp_model)

        # Ensure NLTK resources are available
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
            nltk.data.find('corpora/wordnet')
        except LookupError:
            self.logger.info("Downloading required NLTK resources...")
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('wordnet')

        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

        # Initialize LangChain components if enabled
        self.use_langchain = use_langchain
        if use_langchain:
            try:
                # Adjust this if your version requires a submodule import.
                from langchain_ollama import OllamaLLM
                from langchain.chains import LLMChain
                from langchain.prompts import PromptTemplate

                # Define a prompt template for intent classification
                intent_template = """
                You are an expert system that classifies user queries about AI models into one of these categories:
                - retrieval: asking for general information (e.g. "What is GPT-4?", "Show me models trained on ImageNet")
                - comparison: queries that compare two or more models (must mention at least two model names and comparison keywords like "compare", "vs", "better than")
                - notebook: requests to generate or work with analysis code/notebooks (keywords: create, generate, notebook, Colab, script)
                - image_search: looking for images generated by models, including epoch-based or tag-based filtering (keywords: image, picture, from epoch, tags, colors)
                - metadata: inquiries about model metadata such as creation date, last modified, schema or properties (mentions of months, years, dates, metadata, fields, properties)

                IMPORTANT RULES:
                1. Only classify as comparison if at least two model names are mentioned (exclude dataset or architecture names) and comparison terms are present.
                2. Month/year references like "created in March" or "models from 2022" must be classified as metadata.
                3. Do NOT classify as comparison solely because dataset names (e.g. CIFAR, MNIST) appear.
                4. If keywords for notebook or image_search appear, choose those intents over retrieval.
                5. Retrieval is the default for any general informational query (keywords: find, get, retrieve, what is, show, details) and for any query not matching other intents.
                6. If the query contains the word "using" followed by any recognized model family or architecture (Transformer, GPT, BERT, T5, LLaMA, CLIP, ResNet, ViT, etc.), classify it as retrieval.
                7. Polite prefixes like "please", "kindly", or "could you" do not change the intent.
                8. Terms like "introduce", "describe", "detail", or "give details" should map to retrieval.
                9. If the query contains the keyword "compare" and at least two explicit model identifiers (e.g., phrases starting with "model id" or known model names), classify as comparison.
                10. If the query contains phrases like 'find image', 'find pictures', 'search for images', 'look for images', or any similar phrase about finding/retrieving images, ALWAYS classify as image_search regardless of other criteria.

                EXAMPLES:
                - Query: "Find models using Transformer" -> retrieval
                - Query: "Please find models using Transformer and introduce their details" -> retrieval
                - Query: "Compare GPT-3 vs. GPT-4" -> comparison
                - Query: "Compare model id Multiplication_scriptRNN_ReversedInputString and model id IMDB_scriptRNN" -> comparison
                - Query: "Show me the properties of models from March" -> metadata
                - Query: "Generate a Colab notebook" -> notebook
                - Query: "Show images from epoch 10 with tags 'sunset'" -> image_search
                - Query: "Find images of models trained on CIFAR-10" -> image_search
                - Query: "Please find images of model id ABC" -> image_search
                - Query: "Can you find pictures generated by any models?" -> image_search

                Query: {query}

                Respond ONLY in JSON format:
                {{  
                  "intent": "<intent_name>",
                  "reason": "<short explanation of why this intent was chosen>"
                }}"""

                self.intent_prompt = PromptTemplate(
                    input_variables=["query"],
                    template=intent_template
                )

                # Use the correct model identifier as expected by Ollama.
                self.langchain_llm = OllamaLLM(model=llm_model_name, temperature=0)

                self.intent_chain = self.intent_prompt | self.langchain_llm

                # Define a prompt template for parameter extraction with improved handling of None values
                param_template = """
                Extract structured parameters from this query about AI models.

                Return a JSON object with these keys:
                - filters: Filtering criteria (only include model_id, created_month, created_year, or last_modified_date)
                - limit: Number of results to return (if specified)
                - sort_by: Sorting criteria (if specified)
                - timeframe: Any human-readable time constraint (e.g., "created in March 2025")

                RULES:
                1. Use 'created_month' and 'created_year' to reflect natural date expressions like "March 2025".
                2. Only include those fields if they are mentioned. Do not invent or assume any date.
                3. Never include null, [None], or placeholder values.
                4. Do NOT include filters like architecture, performance, framework, etc.

                EXAMPLES:

                Query: "Find models created in April"  
                →  
                {{  
                  "filters": {{  
                    "created_month": "April"  
                  }},  
                  "limit": null,  
                  "sort_by": null,  
                  "timeframe": "created in April"  
                }}
                
                Query: "List models created in March 2025"  
                →  
                {{  
                  "filters": {{  
                    "created_month": "March",  
                    "created_year": "2025"  
                  }},  
                  "limit": null,  
                  "sort_by": null,  
                  "timeframe": "created in March 2025"  
                }}
                
                Query: {query}
                
                Respond only in this JSON format:
                {{  
                  "filters": {{ ... }},  
                  "limit": ...,  
                  "sort_by": "...",  
                  "timeframe": "..."  
                }}
                """

                self.param_prompt = PromptTemplate(
                    input_variables=["query"],
                    template=param_template
                )

                self.param_chain = self.param_prompt | self.langchain_llm

                self.param_prompt = PromptTemplate(
                    input_variables=["query"],
                    template=param_template
                )

                self.param_chain = self.param_prompt | self.langchain_llm

                self.logger.info("LangChain components initialized successfully")
            except Exception as e:
                self.logger.warning(f"LangChain not available: {e}. Falling back to rule-based parsing")
                self.use_langchain = False

        # Initialize pattern dictionaries for rule-based parsing
        self._init_patterns()

    def _init_patterns(self):
        """Initialize regex patterns and keywords for rule-based parsing."""

        # Parameter extraction patterns

        """
        Regular expression pattern to extract model IDs from text.

        Pattern: r"\b(?:model[-\s_]*id|modelid)(?:\s*[:=]\s*|\s+)([^\s]+)(?:\s|$)"

        This pattern can handle the following cases:
        - model id my_model
        - model id=my_model
        - model id = my_model
        - model id: my_model
        - model id : my_model
        - model-id my_model
        - model-id=my_model
        - model-id = my_model
        - model-id: my_model
        - model-id : my_model
        - model_id my_model
        - model_id=my_model
        - model_id = my_model
        - model_id: my_model
        - model_id : my_model
        - modelid my_model
        - modelid=my_model
        - modelid = my_model
        - modelid: my_model
        - modelid : my_model

        The pattern components:
        1. \b(?:model[-\s_]*id|modelid) - Matches different forms of "model id"
           - model id, model-id, model_id, model--id, model  id, etc.
           - Also matches modelid as a special case
        2. (?:\s*[:=]\s*|\s+) - Matches the delimiter between "model id" and the value
           - Can be a colon or equals with optional surrounding spaces
           - Or just spaces without a delimiter
        3. ([^\s]+) - Captures the model ID (any non-whitespace characters)
        4. (?:\s|$) - Ensures the model ID is followed by whitespace or end of string

        Returns a match object where group(1) contains the extracted model ID.
        """
        self.model_id_pattern = r"\b(?:model[-\s_]*id|modelid)(?:\s*[:=]\s*|\s+)([^\s]+)(?:\s|$)"
        self.metric_pattern = r"(accuracy|loss|perplexity|clip[_-]?score|performance|precision|recall|f1|mae|mse|rmse)"

        self.filter_patterns = {
            "architecture": r"architecture[:\s]+(transformer|cnn|rnn|mlp|diffusion|gan)",
            "framework": r"framework[:\s]+(pytorch|tensorflow|jax)",
            "params": r"(parameters|params)[\s:]+(greater than|less than|equal to|>|<|=)\s*(\d+[KkMmBbTt]?)",
            "date": r"(created|modified|updated)[\s:]+(before|after|between|since)\s+([a-zA-Z0-9_-]+)"
        }

        self.limit_pattern = r"(limit|top|first)\s+(\d+)"
        self.sort_pattern = r"(sort|order)\s+(by|on)\s+([a-zA-Z_]+)\s+(ascending|descending|asc|desc)?"

        # Model name detection - common model families
        self.model_families = [
            "transformer", "gpt", "bert", "t5", "llama", "clip",
            "stable diffusion", "dalle", "cnn", "resnet", "vit",
            "swin", "yolo", "diffusion", "vae", "gan", "bard",
            "mistral", "gemini", "baichuan", "claude", "ernie",
            "chatglm", "falcon", "phi", "qwen", "yi", "bloom", "dqn"
        ]

        # Year extraction patterns with context
        self.created_year_patterns = [
            r"created\s+in\s+(?:the\s+year\s+)?(20\d{2})",
            r"from\s+(?:the\s+year\s+)?(20\d{2})",
            r"developed\s+in\s+(?:the\s+year\s+)?(20\d{2})",
            r"implemented\s+in\s+(?:the\s+year\s+)?(20\d{2})",
            r"built\s+in\s+(?:the\s+year\s+)?(20\d{2})",
            r"made\s+in\s+(?:the\s+year\s+)?(20\d{2})",
            r"dating\s+from\s+(?:the\s+year\s+)?(20\d{2})",
            r"models\s+from\s+(?:the\s+year\s+)?(20\d{2})",
            r"in\s+(?:the\s+year\s+)?(20\d{2})",
            r"year[:\s]+(20\d{2})"
        ]

    def parse_query(self, query_text: str) -> Dict[str, Any]:
        """
        Parse a query to determine intent and parameters.

        Args:
            query_text: The raw query text from the user

        Returns:
            A dictionary containing:
                - intent: The classified intent as a string (not enum)
                - type: Same as intent for backward compatibility
                - parameters: Dictionary of extracted parameters
                - processed_query: The preprocessed query text
        """
        self.logger.debug(f"Parsing query: {query_text}")

        # Preprocess the query
        processed_query = self.preprocess_query(query_text)

        # Classify intent and reason (if any)
        intent, reason = self.classify_intent(query_text)

        # Extract parameters
        parameters = self.extract_parameters(query_text, intent)

        # Convert intent enum to string value for serialization
        intent_str = intent.value if hasattr(intent, 'value') else str(intent)

        result = {
            "intent": intent_str,
            "reason": reason,
            "type": intent_str,  # Add type for backward compatibility
            "parameters": parameters,
            "processed_query": processed_query
        }

        self.logger.info(f"Query parsed: {intent_str} with {len(parameters)} parameters")
        self.logger.debug(f"Parsed result: {result}")

        return result

    def classify_intent(self, query_text: str) -> Union[QueryIntent, tuple[QueryIntent, str]]:
        """
        Classify the intent of a query using the LLM. Rule-based logic is fully embedded in the prompt above.
        """
        if self.use_langchain:
            try:
                # Use invoke() to run the prompt/LLM sequence
                raw_response = self.intent_chain.invoke({"query": query_text})
                if isinstance(raw_response, dict):
                    raw = raw_response.get("text", "")
                else:
                    raw = str(raw_response).strip()

                # Extract JSON blob
                match = re.search(r'{.*}', raw, re.DOTALL)
                if not match:
                    raise ValueError("No JSON object found in LLM response")
                result = match.group(0)

                import json
                parsed = json.loads(result)
                intent = QueryIntent(parsed["intent"])
                reason = parsed.get("reason", "")
                return intent, reason

            except Exception as e:
                self.logger.error(f"Intent classification failed: {e}")

        # Fallback default to retrieval
        return QueryIntent.RETRIEVAL, "Defaulting to retrieval intent."

    def _clean_llm_parameters(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Clean up parameters generated by the LLM to ensure they are safe for ChromaDB.

        Args:
            params: Dictionary of parameters from LLM

        Returns:
            Cleaned parameters
        """
        # Create a deep copy to avoid modifying the original
        cleaned = {k: v for k, v in params.items() if v is not None}

        # Process filters if present
        if "filters" in cleaned and isinstance(cleaned["filters"], dict):
            # Remove None values and "all" model IDs
            cleaned_filters = {}
            for key, value in cleaned["filters"].items():
                # Skip None values
                if value is None:
                    continue

                # Chroma's filter does exact-match. Not including model_id, architecture and framework to filter to prevent finding no result
                if key == "model_id" or key == "architecture" or key == "framework" or key == "dataset":
                    continue

                # Handle dictionary values
                if isinstance(value, dict):
                    valid_dict = {k: v for k, v in value.items() if v is not None}
                    if valid_dict:
                        cleaned_filters[key] = valid_dict
                # Handle list values
                elif isinstance(value, list):
                    if value and not (key == "model_id" and value == ["all"]):
                        cleaned_filters[key] = value
                # Handle scalar values
                else:
                    cleaned_filters[key] = value

            # Replace with cleaned filters or remove if empty
            if cleaned_filters:
                cleaned["filters"] = cleaned_filters
            else:
                del cleaned["filters"]

        return cleaned

    def extract_parameters(self, query_text: str, intent: Optional[QueryIntent] = None) -> Dict[str, Any]:
        """
        Extract parameters from a query based on its intent.

        Args:
            query_text: The query text to extract from
            intent: The query intent, if already classified

        Returns:
            Dictionary of extracted parameters
        """
        if intent is None:
            intent, reason = self.classify_intent(query_text)

        parameters = {}
        query_lower = query_text.lower()

        # Initialize or retrieve existing filters
        filters = parameters.get("filters", {})

        # Defensive enhancement: always try to detect month/year references
        months = [
            "january", "february", "march", "april", "may", "june",
            "july", "august", "september", "october", "november", "december"
        ]
        for month in months:
            if month in query_lower:
                filters["created_month"] = month.capitalize()
                break

        # Extract year using context-aware patterns
        for pattern in self.created_year_patterns:
            year_match = re.search(pattern, query_lower)
            if year_match:
                filters["created_year"] = year_match.group(1)
                break

        # Try LangChain extraction if available
        if self.use_langchain:
            try:
                import json
                raw_result = self.param_chain.invoke({"query": query_text})
                json_start = raw_result.find('{')
                if json_start != -1:
                    json_str = raw_result[json_start:].strip()
                    decoder = json.JSONDecoder()
                    params, idx = decoder.raw_decode(json_str)
                    cleaned_params = self._clean_llm_parameters(params)

                    if "filters" in cleaned_params:
                        filters.update(cleaned_params["filters"])
                        cleaned_params.pop("filters")

                    parameters.update(cleaned_params)
            except Exception as e:
                self.logger.warning(f"LangChain parameter extraction failed: {e}")

        # Rule-based filters (architecture, framework, etc.)
        for filter_name, pattern in self.filter_patterns.items():
            for match in re.finditer(pattern, query_lower):
                if filter_name == "params":
                    filters["params"] = {
                        "operator": match.group(2),
                        "value": match.group(3)
                    }
                elif filter_name == "date":
                    filters["date"] = {
                        "field": match.group(1),
                        "operator": match.group(2),
                        "value": match.group(3)
                    }

        generic_terms = {
            "cnn", "convolutional", "neural network", "deep learning",
            "rnn", "recurrent", "lstm", "transformer", "attention",
            "diffusion", "gan", "generative adversarial", "vae",
            "variational", "autoencoder", "bert", "gpt", "mlp",
            "cifar", "cifar-10", "stl", "stl-10", "oxford", "dqn"
        }

        model_ids = self._extract_model_id_mentions(query_text)
        valid_model_ids = [mid for mid in model_ids if mid.lower() not in generic_terms]

        if valid_model_ids:
            filters["model_id"] = valid_model_ids[0] if len(valid_model_ids) == 1 else valid_model_ids

        if filters:
            parameters["filters"] = filters

        # Result limit
        limit_match = re.search(self.limit_pattern, query_lower)
        if limit_match:
            parameters["limit"] = int(limit_match.group(2))

        # Sort
        sort_match = re.search(self.sort_pattern, query_lower)
        if sort_match:
            parameters["sort_by"] = {
                "field": sort_match.group(3),
                "order": sort_match.group(4) if sort_match.group(4) else "descending"
            }

        # Intent-specific
        if intent == QueryIntent.COMPARISON:
            parameters.update(self._extract_comparison_parameters(query_text))
        elif intent == QueryIntent.NOTEBOOK:
            parameters.update(self._extract_notebook_parameters(query_text))
        elif intent == QueryIntent.IMAGE_SEARCH:
            parameters.update(self._extract_image_parameters(query_text, valid_model_ids))

        return parameters

    def _extract_model_id_mentions(self, query_text: str) -> List[str]:
        """
        Extract mentions of model IDs or names from query text.

        Returns:
            List of valid model identifiers (excluding known dataset names and architectures)
        """
        model_ids = []

        # Common datasets to exclude from model_id
        COMMON_DATASETS = {
            "cifar", "cifar10", "cifar-10", "cifar100", "imagenet", "mnist",
            "fashion-mnist", "coco", "cityscapes", "voc", "svhn", "celeba",
            "librispeech", "wikitext", "squad", "glue", "webtext", "laion", "ms coco"
        }

        # Generic architectures to exclude
        GENERIC_ARCH = {
            "cnn", "rnn", "lstm", "transformer", "gan", "vae", "mlp",
            "diffusion", "autoencoder", "bert", "gpt"
        }

        model_id_pattern = re.compile(
            self.model_id_pattern,
            flags=re.IGNORECASE
        )
        # Try to extract explicit model_id mentions
        for match in model_id_pattern.finditer(query_text):
            model_id = match.group(1)  # e.g. "Multiplication_scriptRNN_ReversedInputString"
            if model_id.lower() not in GENERIC_ARCH and model_id.lower() not in COMMON_DATASETS:
                model_ids.append(model_id)
        print(f"Extracted model id(s) {model_ids} from query {query_text}")

        # Deduplicate
        return list(set(model_ids))

    def _extract_comparison_parameters(self, query_text: str) -> Dict[str, Any]:
        """
        Extract comparison-specific parameters.

        Args:
            query_text: The query text to extract from

        Returns:
            Dictionary of comparison parameters
        """
        params = {}

        # Extract comparison dimensions
        dimensions = []
        dimension_pattern = r"(compare|comparing|comparison) (on|by|in terms of|regarding) ([\w\s,]+)"
        match = re.search(dimension_pattern, query_text.lower())
        if match:
            # Split by commas or 'and' and clean up
            dims = re.split(r',|\sand\s', match.group(3))
            dimensions = [dim.strip() for dim in dims if dim.strip()]

        if dimensions:
            params["comparison_dimensions"] = dimensions

        # Extract visualization preference
        visualization_pattern = r'(show|display|visualize|plot|graph|chart)'
        if re.search(visualization_pattern, query_text.lower()):
            params["visualize"] = True

        return params

    def _extract_notebook_parameters(self, query_text: str) -> Dict[str, Any]:
        """
        Extract notebook generation specific parameters.

        Args:
            query_text: The query text to extract from

        Returns:
            Dictionary of notebook parameters
        """
        params = {}

        # Extract analysis type
        analysis_types = []
        analysis_pattern = r"(analyze|analysis|examine|study|investigate) ([\w\s,]+)"
        match = re.search(analysis_pattern, query_text.lower())
        if match:
            # Split by commas or 'and' and clean up
            types = re.split(r',|\sand\s', match.group(2))
            analysis_types = [t.strip() for t in types if t.strip()]

        if analysis_types:
            params["analysis_types"] = analysis_types

        # Check for dataset mention
        dataset_pattern = r"(dataset|data)[:\s]+([\w\s-]+)"
        match = re.search(dataset_pattern, query_text.lower())
        if match:
            params["dataset"] = match.group(2).strip()

        # Check for resource constraints
        resource_pattern = r"(using|with) ([\w\s]+) (resources|gpu|memory|cpu)"
        match = re.search(resource_pattern, query_text.lower())
        if match:
            params["resources"] = match.group(2).strip()

        return params

    def _extract_image_parameters(self, query_text: str, valid_model_ids: list) -> Dict[str, Any]:
        """
        Extract image search specific parameters.

        Args:
            query_text: The query text to extract from

        Returns:
            Dictionary of image search parameters
        """
        params = {}
        query_lower = query_text.lower()

        # Determine search type based on query patterns
        if re.search(r"highest\s+epoch|latest\s+epoch", query_lower):
            params["search_type"] = "highest_epoch"
        elif re.search(r"epoch\s*[=:]\s*\d+", query_lower) or re.search(r"from\s+epoch\s+\d+", query_lower):
            params["search_type"] = "epoch"
            # Extract epoch number
            epoch_match = re.search(r"epoch\s*[=:]\s*(\d+)", query_lower) or re.search(r"from\s+epoch\s+(\d+)",
                                                                                       query_lower)
            if epoch_match:
                params["epoch"] = int(epoch_match.group(1))
        elif re.search(r"tag[s]?\s*[=:]\s*|with\s+tags?\s+", query_lower):
            params["search_type"] = "tag"
            # Extract tags
            tags_pattern = r"tag[s]?\s*[=:]\s*\"?([^\"]+)\"?|with\s+tags?\s+\"?([^\"]+)\"?"
            tags_match = re.search(tags_pattern, query_lower)
            if tags_match:
                # Get whichever group matched
                tags_str = tags_match.group(1) if tags_match.group(1) else tags_match.group(2)
                # Split by commas or 'and'
                tags = re.split(r',|\sand\s', tags_str)
                params["tags"] = [tag.strip() for tag in tags if tag.strip()]
                params["require_all"] = "all" in query_lower and "tags" in query_lower
        elif re.search(r"color[s]?\s*[=:]\s*|with\s+color[s]?\s+", query_lower):
            params["search_type"] = "color"
            # Extract colors
            colors_pattern = r"color[s]?\s*[=:]\s*\"?([^\"]+)\"?|with\s+color[s]?\s+\"?([^\"]+)\"?"
            colors_match = re.search(colors_pattern, query_lower)
            if colors_match:
                # Get whichever group matched
                colors_str = colors_match.group(1) if colors_match.group(1) else colors_match.group(2)
                # Split by commas or 'and'
                colors = re.split(r',|\sand\s', colors_str)
                params["colors"] = [color.strip() for color in colors if color.strip()]
        elif re.search(r"date\s*[=:]\s*|created\s+(on|in)\s+", query_lower):
            params["search_type"] = "date"
            # Extract date components
            date_filter = {}

            # Look for year
            year_match = re.search(r"(20\d{2})", query_lower)
            if year_match:
                date_filter["created_year"] = year_match.group(1)

            # Look for month
            months = ["january", "february", "march", "april", "may", "june",
                      "july", "august", "september", "october", "november", "december"]
            for i, month in enumerate(months, 1):
                if month in query_lower:
                    date_filter["created_month"] = str(i).zfill(2)  # "01" for January, etc.
                    break

            params["date_filter"] = date_filter
        elif re.search(r"content\s*[=:]\s*|subject\s*[=:]\s*|scene\s*[=:]\s*", query_lower):
            params["search_type"] = "content"
            # Extract content filter components
            content_filter = {}

            # Subject type
            subject_match = re.search(r"subject\s*[=:]\s*\"?([^\"]+)\"?", query_lower)
            if subject_match:
                content_filter["subject_type"] = subject_match.group(1).strip()

            # Scene type
            scene_match = re.search(r"scene\s*[=:]\s*\"?([^\"]+)\"?", query_lower)
            if scene_match:
                content_filter["scene_type"] = scene_match.group(1).strip()

            params["content_filter"] = content_filter
        elif valid_model_ids is not None and len(valid_model_ids) > 0:
            params["search_type"] = "model_id"
        else:
            # Default to similarity search
            params["search_type"] = "similarity"

        # Extract prompt terms (for similarity search)
        prompt_pattern = r"(prompt|prompts|text)[:\s]+[\"']?([\w\s,]+)[\"']?"
        match = re.search(prompt_pattern, query_lower)
        if match:
            params["prompt_terms"] = match.group(2).strip()

            # If search_type is similarity, set query_text as well
            if params.get("search_type") == "similarity":
                params["query_text"] = match.group(2).strip()

        # Extract style tags
        style_pattern = r"(style|type|category|look)[:\s]+[\"']?([\w\s,]+)[\"']?"
        match = re.search(style_pattern, query_lower)
        if match:
            # Split by commas and clean up
            styles = re.split(r',|\sand\s', match.group(2))
            params["style_tags"] = [style.strip() for style in styles if style.strip()]

        # Extract resolution preference
        resolution_pattern = r"(resolution|size|dimensions)[:\s]+(\d+)\s*[x×]\s*(\d+)"
        match = re.search(resolution_pattern, query_lower)
        if match:
            params["resolution"] = {
                "width": int(match.group(2)),
                "height": int(match.group(3))
            }

        # Extract limit for results
        limit_pattern = r"(limit|top|first)\s+(\d+)"
        match = re.search(limit_pattern, query_lower)
        if match:
            params["limit"] = int(match.group(2))
        else:
            # Default limit
            params["limit"] = 10

        return params

    def preprocess_query(self, query_text: str) -> str:
        """
        Preprocess a query for searching.

        Args:
            query_text: The raw query text

        Returns:
            Preprocessed query text
        """
        # Basic cleaning
        query_text = query_text.strip()

        # Use spaCy for tokenization and lemmatization
        doc = self.nlp(query_text)

        # Enhanced token handling
        processed_tokens = []
        skip_indices = set()

        # First pass: identify entities to preserve as-is
        for ent in doc.ents:
            if ent.label_ in ["PRODUCT", "ORG", "GPE", "PERSON", "WORK_OF_ART"]:
                for i in range(ent.start, ent.end):
                    skip_indices.add(i)
                processed_tokens.append(ent.text)

        # Second pass: handle noun phrases (potential model names)
        for chunk in doc.noun_chunks:
            # Check if any of the model families appear in this noun chunk
            contains_model = any(family in chunk.text.lower() for family in self.model_families)
            if contains_model:
                # Check if all tokens in this chunk are already skipped
                all_skipped = all(i in skip_indices for i in range(chunk.start, chunk.end))
                if not all_skipped:
                    # Add all tokens in this chunk to skip indices
                    for i in range(chunk.start, chunk.end):
                        skip_indices.add(i)
                    processed_tokens.append(chunk.text)

        # Third pass: process remaining tokens
        for i, token in enumerate(doc):
            if i not in skip_indices:
                # Skip stopwords, punctuation, and spaces
                if not token.is_stop and not token.is_punct and not token.is_space and len(token.text.strip()) > 1:
                    # Use lemma for standardization
                    processed_tokens.append(token.lemma_.lower())

        # Join all processed tokens
        return " ".join(processed_tokens)
